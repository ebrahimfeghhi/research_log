{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9/17/25\n",
    "\n",
    "\n",
    "**<span style=\"color:#B3E5FC\">Background Description</span>**\n",
    "\n",
    "On the Brain-to-Text '24 Dataset, impressive gains in accuracy were made using the following techniques. \n",
    "\n",
    "1. Using an ensemble of 10 neural networks with different seeds. \n",
    "\n",
    "2. Decoding the outputs of each of these seeds with a 5-gram language model using beam search with a second pass rescoring stage applied.\n",
    "\n",
    "3. Obtaining the top 100 beams from each seed and selecting the best beam with OPT 6.7B resulting in a single, best hypothesis for each seed. \n",
    "\n",
    "4. Fine-tuning a LLM to produce the intended transcription given the best hypothesis from each seed. \n",
    "\n",
    "**<span style=\"color:#B3E5FC\">Areas for Improvement</span>**\n",
    "\n",
    "1. The 5-gram LM + second pass rescoring requires very intensive computational resources, specifically a server with \n",
    "over 300 GB of RAM. Furthermore, it is unclear if latency is an issue with the 5-gram LM. It appears that Card et al., 2025 \n",
    "used a 5-gram LM + Rescoring + OPT 6.7B for online evaluations, so this may not be a critical issue.\n",
    "\n",
    "2. LLM fine-tuning relies on the entire sentence being decoded, and no previous method has applied it in a streaming fashion. \n",
    "\n",
    "3. The acoustic model is crafted based on PER, whereas the metric of interest is WER. Given the complexity and large resource demands of this setup, optimizing for WER in the creation of the acoustic model during training appears challenging (loading the 5-gram LM, latency costs for decoding the entire validation set, the LLM can only be fine-tuned after the acoustic model has finished training). This is not ideal because PER is not a perfect indicator of WER, which may lead to imperfect hyperparameter optimization for the acoustic model.\n",
    "    * The relationship between WER and PER appears to be complex, and modulated by validation CTC loss. Validation CTC loss means that the sum over the probability of valid alignments is high, whereas PER indicates the edit distance between the correct and decoded sequences after CTC greedy decoding rules are applied.\n",
    "    * A data point that would be valuable to have is what the WER is for a consistency regularized CTC model that is trained for 250 epochs. Such a model appears to obtain a lower PER and validation CTC loss relative to the current best model I have.\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Model Name                        |   N |    PER |   CTC Loss |   3-gram WER |\n",
      "|:----------------------------------|----:|-------:|-----------:|-------------:|\n",
      "| Transformer 600 epochs + 7 layers |   4 | 0.1408 |     0.8555 |       0.1769 |\n",
      "| Transformer 250 Epochs + 5 layers |   4 | 0.1566 |     0.7265 |       0.1708 |\n",
      "| Transformer with CR-CTC 0.2       |   1 | 0.1431 |     0.5824 |       0.1689 |\n"
     ]
    }
   ],
   "source": [
    "from helper_functions import process_and_display_results\n",
    "\n",
    "process_and_display_results(['research_data/neurips_transformer_time_masked.pkl', 'research_data/transformer_short_training_fixed.pkl', 'research_data/time_masked_transformer_cr-ctc_0.2.pkl'], \n",
    "                            ['Transformer 600 epochs + 7 layers', 'Transformer 250 Epochs + 5 layers', \"Transformer with CR-CTC 0.2\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
